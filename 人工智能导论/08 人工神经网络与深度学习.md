# 8.1 神经元与神经网络
## 8.1.1 生物神经元结构
![](assets/8.1.1.png)
## 8.1.2 神经元数学模型
![](assets/8.1.2.png)
神经元数学模型可由以下三部分组成：
- 加权求和
- 线性动态系统
- 非线性函数

$y_i(t)$为第i个神经元的输出，$\theta_{i}$为第i个神经元的阈值，$u_k(t)$为外部输入$a_{ij}，b_{ik}$为权值
$$
v_{i}(t)=\sum_{j=1}^{N}a_{ij}y_{j}(t)+\sum_{k=1}^{M}b_{ik}u_{k}(t)-\theta_{i}
$$
- 矩阵形式
$$
v(t)=Ay(t)+Bu(t)−θ
$$

线性环节的传递函数描述为
$$
X_{i}(s)=H(s)V_{i}(s)
$$
## 8.1.3 神经网络的结构与工作方式
### 1. 神经网络的结构
1. 前馈型
2. 反馈型
### 2. 神经网络的工作方式
## 8.1.4 神经网络的学习
# 8.2 BP神经网络
## 8.2.1 BP神经网络的结构
1. **分层结构​**​
    
    - ​**​输入层​**​：负责接收外部数据。神经元个数由输入数据的特征维度决定（如图像的像素数、数据的指标数）。该层仅传递数据，不进行加权求和与激活。
        
    - ​**​隐藏层​**​：介于输入层和输出层之间，是神经网络的“大脑”，负责进行特征抽象和转换。可以有一层或多层（超过一层可称为深度神经网络）。每个神经元都会进行​**​加权求和​**​并通过​**​激活函数​**​产生输出。
        
    - ​**​输出层​**​：网络最终的输出结果。神经元个数由任务决定（如分类的类别数、回归值的个数）。同样，每个神经元也会进行加权求和与激活。
    
2. ​**​前馈连接​**​
    
    信号​**​单向​**​从输入层流向输出层，同一层的神经元之间没有连接，每个神经元只与下一层的所有神经元全连接。这种结构就是“前馈”。
    
3. ​**​权重与偏置​**​
    
    连接每个神经元的线都有一个​**​权重​**​，每个神经元（除输入层外）都有一个​**​偏置​**​。这些权重和偏置就是神经网络需要学习的​**​参数​**​。训练BP网络的过程，本质上就是通过反向传播算法来调整这些权重和偏置的值，以使网络的输出接近真实值。
    
4. ​**​激活函数​**​
    
    每个隐藏层和输出层的神经元都包含一个​**​激活函数​**​（如Sigmoid, ReLU, Tanh）。它的作用是引入非线性，使得网络可以拟合非常复杂的非线性关系。如果没有激活函数，无论网络有多少层，都等价于一个单层线性模型。
## 8.2.2 BP学习算法
**万能近似定理​**​ 在数学上证明了：​**​一个只包含一个隐藏层的前馈神经网络，只要隐藏层有足够多的神经元，就可以以任意精度近似任何从一个有限维空间到另一个有限维空间的连续函数。​**​

简单来说：

> 无论你要拟合的函数有多么复杂（比如识别猫狗、预测股价、翻译语言），总存在一个足够大的单隐藏层神经网络，可以像“万能钥匙”一样，无限逼近这个函数。

对于任意给定的连续函数 f:[0,1]n→Rm和任意精度 ϵ>0，

​**​存在一个三层前馈神经网络​**​ N，其结构如下：

1. ​**​输入层​**​：包含 n个神经元，接收输入向量 x=(x1​,x2​,...,xn​)T。
    
2. ​**​单隐藏层​**​：包含 H个神经元（H是一个有限的、足够大的整数）。
    
3. ​**​输出层​**​：包含 m个神经元，产生输出向量 y^​=(y^​1​,y^​2​,...,y^​m​)T。
    
4. ​**​隐藏层激活函数​**​ σ：是一个非线性的、有界的、非常数的连续函数（例如Sigmoid函数）。
    

使得对于所有定义域内的输入 x，神经网络的输出 y^​与目标函数 f(x)的差异小于精度要求：

$$x∈[0,1]nsup​∥N(x)−f(x)∥<ϵ$$

这里，N(x)表示网络对输入 x的计算结果。
BP学习算法使通过反向学习过程使误差最小，因此选择目标函数为
$$
minJ=\frac{1}{2}\sum_{j=1}^{P_{m}}(y_{j}^{m}-y_{sj})^2
$$
神经网络权值的修正量为
$$
\Delta w_{ij}^{k-1}=-\epsilon \ \frac{\partial J}{\partial w_{ij}^{k-1}} (\epsilon > 0)
$$
其中$\epsilon$为学习步长，一般小于0.5
### BP算法的推导
#### 一、网络结构及符号定义
假设我们有一个多层前馈神经网络，包含输入层、一个或多个隐藏层和输出层。我们使用以下符号：

- L：网络的总层数（输入层为第 0 层，输出层为第 L 层）。
    
- k：层的索引，k=0,1,…,L。
    
- $n_k​$：第 k 层的神经元个数。
    
- $x_i^k$​：第 k 层第 i 个神经元的输入（即加权求和后的值）。
    
- $y_i^k$​：第 k 层第 i 个神经元的输出（即经过激活函数后的值）。
    
- $w_ij^k​$：第 k 层第 i 个神经元到第 k+1 层第 j 个神经元的连接权值。
    
- $b_j^{k+1}​$：第 k+1 层第 j 个神经元的偏置。
    
- f(⋅)：激活函数，通常使用 Sigmoid 函数、ReLU 函数等。
    

网络的前向传播过程为：
$$
x_{j}^{k+1}​=i=1\sum _{i=1}^{n_{k}}w_{ij}^{k}y_{i}^{k}​+b_{j}^{k+1}
$$
$$​
y_{j}^{k+1}​=f(x_j^{k+1}​)
$$
#### 二、误差反向传播算法推导
BP 算法的核心思想是：通过计算误差函数对各个权值的偏导数，然后利用梯度下降法来更新权值，使得误差函数最小化。

##### 1. 输出层的误差计算

首先，我们定义输出层的误差信号。对于输出层第 j 个神经元，其误差信号为：

$$
\delta_{j}^{L}=\frac{\partial J}{\partial x_{j}^{L}}.
$$

根据链式法则，我们有：

$$
\delta_{j}^{L}=\frac{\partial J}{\partial y_{j}^{L}}\cdot\frac{\partial y_{j}^{L}}{\partial x_{j}^{L}}
$$

由于$J=\dfrac{1}{2}\sum_{j=1}^{P_{m}}\bigl(y_{j}^{L}-y_{sj}\bigr)^{2}$，且  $P_{m}=n_{L}$，则：

$$
\frac{\partial J}{\partial y_{j}^{L}}=\bigl(y_{j}^{L}-y_{sj}\bigr).
$$

又因为$y_{j}^{L}=f\!\bigl(x_{j}^{L}\bigr)$，所以：

$$
\frac{\partial y_{j}^{L}}{\partial x_{j}^{L}}=f'\!\bigl(x_{j}^{L}\bigr).
$$

因此，输出层的误差信号为：

$$
\delta_{j}^{L}=\bigl(y_{j}^{L}-y_{sj}\bigr)\,f'\!\bigl(x_{j}^{L}\bigr).
$$

##### 2. 隐藏层的误差计算

对于隐藏层（第 k 层，k=1,2,…,L−1），其误差信号可以通过下一层（第 k+1 层）的误差信号反向传播得到。

定义隐藏层第 k 层第 i 个神经元的误差信号为：
$$
\delta_{i}^{k}=\frac{\partial J}{\partial x_{i}^{k}}.
$$
根据链式法则，我们有：
$$
\delta_{i}^{k}=\sum_{j=1}^{n_{k+1}}\frac{\partial J}{\partial x_{j}^{k+1}}\cdot
\frac{\partial x_{j}^{k+1}}{\partial y_{i}^{k}}\cdot
\frac{\partial y_{i}^{k}}{\partial x_{i}^{k}}.
$$
我们知道：

- $\dfrac{\partial J}{\partial x_{j}^{k+1}}=\delta_{j}^{k+1}$
    
- $x_{j}^{k+1}=\sum_{i=1}^{n_{k}}w_{ij}^{k}\,y_{i}^{k}+b_{j}^{k+1}$,
		故 $\dfrac{\partial x_{j}^{k+1}}{\partial y_{i}^{k}}=w_{ij}^{k}$
    
- $y_{i}^{k}=f\!\bigl(x_{i}^{k}\bigr)$,
        故 $\dfrac{\partial y_{i}^{k}}{\partial x_{i}^{k}}=f'\!\bigl(x_{i}^{k}\bigr)$
    

因此，隐藏层的误差信号为：

$$
\delta_{i}^{k}=\Biggl(\,\sum_{j=1}^{n_{k+1}}\delta_{j}^{k+1}\,w_{ij}^{k}\Biggr)\,f'\!\bigl(x_{i}^{k}\bigr)
$$

##### 3. 权值修正量的计算

现在，我们来计算误差函数对权值的偏导数。

对于第 k−1 层第 i 个神经元到第 k 层第 j 个神经元的连接权值  $w_{ij}^{k-1}$，我们有：

$$
\frac{\partial J}{\partial w_{ij}^{k-1}}
=\frac{\partial J}{\partial x_{j}^{k}}\cdot
\frac{\partial x_{j}^{k}}{\partial w_{ij}^{k-1}}.
$$

我们知道：

-  $\dfrac{\partial J}{\partial x_{j}^{k}}=\delta_{j}^{k}$
    
- $x_{j}^{k}=\sum_{i=1}^{n_{k-1}}w_{ij}^{k-1}\,y_{i}^{k-1}+b_{j}^{k}$,
        故 $\dfrac{\partial x_{j}^{k}}{\partial w_{ij}^{k-1}}=y_{i}^{k-1}$
    

因此，误差函数对权值的偏导数为：

$$
\frac{\partial J}{\partial w_{ij}^{k-1}}=\delta_{j}^{k}\,y_{i}^{k-1}.
$$

根据梯度下降法，权值的修正量为：

$$
\Delta w_{ij}^{k-1}=-\epsilon\,\frac{\partial J}{\partial w_{ij}^{k-1}}=-\epsilon\,\delta_{j}^{k}\,y_{i}^{k-1}.
$$

类似地，对于偏置 bjk​，其修正量为：

$$
\Delta b_{j}^{k}=-\epsilon\,\frac{\partial J}{\partial b_{j}^{k}}=-\epsilon\,\delta_{j}^{k}.
$$
## 8.2.3 BP学习算法的实现
1. **初始化**  
    对所有权值与偏置赋小随机数：
$$
w_{ij}^{k}\sim\mathcal{U}(-\alpha,\alpha),\quad
$$
$$
b_{j}^{k+1}\sim\mathcal{U}(-\alpha,\alpha),\quad
\alpha\ll 1.
$$
2. **前向传播**（对当前样本）  
    逐层计算
$$  x_{j}^{k+1}=\sum_{i=1}^{n_{k}}w_{ij}^{k}\,y_{i}^{k}+b_{j}^{k+1},
$$
$$
y_{j}^{k+1}=f\!\bigl(x_{j}^{k+1}\bigr)
$$
 直至得到输出层结果 yjL​。
    
3. **输出层误差**
    
$$
 \delta_{j}^{L}=\bigl(y_{j}^{L}-y_{sj}\bigr)\,f'\!\bigl(x_{j}^{L}\bigr).
$$
4. **反向传播误差**  
    对 k=L−1,L−2,…,1 逐层计算
    
$$
\delta_{i}^{k}=\Biggl(\,\sum_{j=1}^{n_{k+1}}\delta_{j}^{k+1}\,w_{ij}^{k}\Biggr)\,f'\!\bigl(x_{i}^{k}\bigr).
$$
5. **权值与偏置更新**
    
$$
   \Delta w_{ij}^{k-1}=-\epsilon\,\delta_{j}^{k}\,y_{i}^{k-1},\quad
   w_{ij}^{k-1}\leftarrow w_{ij}^{k-1}+\Delta w_{ij}^{k-1};
$$
$$
   \Delta b_{j}^{k}=-\epsilon\,\delta_{j}^{k},\quad
   b_{j}^{k}\leftarrow b_{j}^{k}+\Delta b_{j}^{k}.
$$
6. **循环控制**
	- 对训练集所有样本执行 2–5（完成 1 个 epoch）。
	- 重复 epoch 直至 $J=\frac12\sum_{j=1}^{P_{m}}\bigl(y_{j}^{L}-y_{sj}\bigr)^{2}\le J_{\text{target}}$ 或达到最大迭代次数。

7. **返回** 训练后的所有权值 ${wijk​}$ 与偏置 ${bjk​}$。

![](assets/8.2.3.jpg)

# 8.3 BP神经网络在模式识别中的应用
- 模式识别研究用计算机模拟生物、人的感知，对模式信息，如图像、文字、语音等，进行识别和分类。
- 传统人工智能的研究部分地显示了人脑的归纳、推理等智能。但是，对于人类底层的智能，如视觉、听觉、触觉等方面，现代计算机系统的信息处理能力还不如一个幼儿园的孩子。
- 神经网络模型模拟了人脑神经系统的特点：处理单元的广泛连接；并行分布式信息储存、处理；自适应学习能力等。
- 神经网络模式识别方法具有较强的容错能力、自适应学习能力、并行信息处理能力。
# 8.4 Hopfield 神经网络及其改进  
## 8.4.1 网络结构与能量函数  
- **单节点模型**  
  每个神经元输出二元值：  
  $$  
  x_i \in \{ -1, +1 \} \quad \text{或} \quad \{ 0, 1 \}  
  $$  
  更新规则（异步串行）：  
  $$  
  x_i \leftarrow \text{sgn}\left( \sum_{j \neq i} w_{ij} x_j + \theta_i \right)  
  $$  

- **连接矩阵**  
  权值对称且无自环：  
  $$  
  w_{ij} = w_{ji}, \quad w_{ii} = 0  
  $$  

- **能量函数（Lyapunov）**  
  $$  
  E = -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} x_i x_j - \sum_{i=1}^{N} \theta_i x_i  
  $$  
  异步更新必使 $E$ 单调下降 $\Rightarrow$ 网络终会收敛到稳定点（吸引子）。

## 8.4.2 标准学习规则（Hebb 外积法）  
给定 $P$ 个待记忆模式 $\xi^{\mu} = (\xi_1^{\mu}, \dots, \xi_N^{\mu})$：  
$$  
w_{ij} = \frac{1}{N} \sum_{\mu=1}^{P} \xi_i^{\mu} \xi_j^{\mu} \quad (i \neq j)  
$$  
- 无偏置时 $\theta_i = 0$。  
- 存储容量：随机模式且 $N \to \infty$ 时，最大可稳定存储  
  $$  
  P_{\max} \approx 0.14 N  
  $$  
  超过后伪吸引子（串扰）急剧增加。

## 8.4.3 典型改进算法  
1. **伪逆学习（Projection Rule）**  
   令模式矩阵 $\Xi = [\xi^1, \dots, \xi^P]$，则  
   $$  
   \mathbf{W} = \frac{1}{N} \Xi \Xi^+ \quad (\Xi^+ \text{为伪逆})  
   $$  
   可保证所有训练模式成为固定点，容量提升至 $P \le N$。  

2. **增强学习（Iterative Learning）**  
   重复“误差修正”直至所有模式收敛：  
   $$  
   \Delta w_{ij}^{(t)} = \eta \sum_{\mu=1}^{P} (\xi_i^{\mu} - x_i^{\mu}) \xi_j^{\mu}  
   $$  

3. **连续 Hopfield（Continuous-Time, 1984）**  
   神经元输出连续化（tanh 或 sigmoid），微分方程：  
   $$  
   C_i \frac{d u_i}{d t} = -\frac{u_i}{R_i} + \sum_{j} w_{ij} g(u_j) + I_i  
   $$  
   能量函数：  
   $$  
   E = -\frac{1}{2} \sum_{i,j} w_{ij} g(u_i) g(u_j) + \sum_i \frac{1}{R_i} \int_0^{g(u_i)} g^{-1}(x) dx - \sum_i I_i g(u_i)  
   $$  
   仍满足 $dE/dt \le 0$。

4. **暂态混沌神经网络（TCNN）**  
   在状态方程中加入自反馈慢变项与混沌噪声，利用混沌遍历避开局部极小，再逐渐退火到 Hopfield 动力学，显著改善组合优化解的质量。

---

# 8.5 Hopfield 神经网络的应用  
## 8.5.1 联想记忆（Associative Memory）  
- **自联想**：输入含噪或部分缺失模式，网络收敛到最相似存储样本。  
  流程：  
  1. 权值按 Hebb/伪逆规则存储样本；  
  2. 给定探针模式 $\tilde{\xi}$ 作为初始状态；  
  3. 异步迭代更新，输出稳定态即为回忆结果。  

- **异联想（Bidirectional Associative Memory, BAM）**  
  扩展为双层双向网络，实现 $X \leftrightarrow Y$ 映射，容量 $P \le \min(N,M)$。

## 8.5.2 组合优化问题  
将约束和目标编码为能量函数，网络收敛即对应可行/最优解。  

1. **旅行商问题（TSP）**  
   - 编码：$N$ 城市 $\times N$ 次序，共 $N^2$ 个神经元，输出矩阵每行每列仅一个“1”。  
   - 能量项：  
     $$  
     E = A \sum_x \sum_i \sum_{j \neq i} y_{xi} y_{xj} \quad \text{(行约束)}  
     $$  
     $$  
     + B \sum_i \sum_x \sum_{y \neq x} y_{xi} y_{yi} \quad \text{(列约束)}  
     $$  
     $$  
     + C \left( \sum_x \sum_i y_{xi} - N \right)^2 \quad \text{(全局约束)}  
     $$  
     $$  
     + D \sum_x \sum_{y \neq x} \sum_i d_{xy} y_{xi} (y_{y,i+1} + y_{y,i-1}) \quad \text{(路径长度)}  
     $$  
   - 结果：10–30 城市规模可在毫秒级收敛，但解质量随规模下降，需配合退火或增强学习。  

2. **图划分/调度/任务分配**  
   同样把约束写成惩罚项，系数需精细调节或采用 Lagrange-Hopfield 混合网络。

## 8.5.3 信号恢复与重建  
- 稀疏信号：把 L 0 约束转化为能量惩罚，用连续 Hopfield 求解。  
- 图像去噪：存储“干净块”作为吸引子，输入噪声块后迭代收敛实现滤波。  

## 8.5.4 硬件实现与神经形态芯片  
- 权值矩阵用交叉阵列（Crossbar）实现，忆阻器（Memristor）直接充当 $w_{ij}$，完成“存储-计算”融合，能耗比传统 CPU 低 2–3 量级。  
- 典型芯片：IBM TrueNorth、Intel Loihi 均借鉴 Hopfield 动力学实现局部关联记忆与实时优化。

## 8.5.5 小结  
Hopfield 网络以“能量下降”为核心，为联想记忆和实时优化提供了硬件友好、数学严格的框架；借助改进学习规则、混沌退火及神经形态器件，仍在小样本回忆、低功耗优化和边缘计算场景发挥独特作用。

# 8.6 卷积神经网络与深度学习
 BP算法需要巨量的计算量，而局部敏感和方向选择的神经元可以降低计算量
## 8.6.1 卷积神经网络的结构
卷积神经网络（Convolutional Neural Network, CNN）通过局部感受野、权值共享和降采样三种机制，显著减少网络参数并降低训练计算量，同时保留平移不变性。其典型拓扑依次为：

1. 输入层  
    直接接收原始图像或语音时频图等二维/三维数据。
    
2. 卷积层（Convolutional Layer）
    
    - 卷积核（Filter）尺寸远小于输入图，每个核在整张图上滑动做离散卷积，输出一张特征图（Feature Map）。
        
    - 同一特征图共享一套权值，参数远少于全连接。
        
    - 常用激活函数：ReLU、Leaky-ReLU。
        
3. 池化层（Pooling Layer）  
    对特征图进行降采样（Max / Mean Pooling），进一步压缩维度、增强鲁棒性。
    
4. 全连接层（Fully-connected Layer）  
    经过若干卷积-池化堆叠后，将三维特征图展平，接入传统全连接网络完成分类或回归。
    
5. 输出层  
    任务相关：Softmax（多类）、Sigmoid（多标签）、Linear（回归）等。
    

整体前向流程可概括为  
Input → [[Conv → ReLU] × N → Pool?] × M → Flatten → FC → Output  
反向传播仍采用 BP，但梯度需沿“卷积-池化”结构回传，并服从权值共享约束。
# 8.7 生成对抗网络及其应用
## 基本思想

生成对抗网络（Generative Adversarial Network, GAN）由 Goodfellow 于 2014 年提出，核心是让两个网络在零和博弈中共同进化：

- 生成器 G(z)：接收随机噪声 z，输出伪样本 G(z)。
    
- 判别器 D(x)：对任意样本 x 输出其为真实数据的概率 D(x)∈[0,1]。
    

目标函数  
$min_G max_D V(D,G)=E_{x∼p_data}[log D(x)]+E_{z∼p_z}[log(1−D(G(z)))]$

训练步骤（交替进行）

1. 固定 G，更新 D：最大化判别准确率；
    
2. 固定 D，更新 G：最小化 log(1−D(G(z)))（等价最大化 D 被骗的概率）。
    

## 改进与变体

- DCGAN：引入卷积结构，生成高质量图像。
    
- Conditional GAN：额外输入标签 y，实现定向生成。
    
- WGAN / WGAN-GP：用 Wasserstein 距离替代 JS 散度，缓解训练不稳定与模式崩塌。
    
- StyleGAN 系列：分层控制生成细节，实现 1024×1024 高清人脸合成。
    

## 应用

1. 图像合成：人脸、场景、艺术风格创作。
    
2. 数据增强：医学影像、小样本目标检测。
    
3. 图像修复与超分辨率：缺失区域补全、低清转高清。
    
4. 跨域迁移：CycleGAN 实现“马↔斑马”、“照片↔油画”无配对转换。
    
5. 语音与文本：TTS 声码器、对话生成、文本到图像（DALL·E、Stable Diffusion）。
# 8.8 小结
1. 从生物神经元到 M-P 模型，再到多层前馈网络，神经网络通过“加权求和 + 非线性激活”统一了特征提取与分类器设计。
    
2. BP 算法以梯度下降为核心，把输出误差反向传播到各隐层，实现权值自动修正；万能近似定理保证了足够宽的单隐层网络即可拟合任意连续函数，但深度结构在参数效率与特征层次化表达上更具优势。
    
3. 传统全连接网络参数冗余，对高维局部相关数据（图像、语音）效率低；CNN 引入局部感受野、权值共享和池化，显著降低计算量并保留平移不变性，成为深度学习在视觉任务上的突破口。
    
4. 以 GAN 为代表的生成模型，通过对抗训练机制，使神经网络不仅能判别，更能创造，推动了图像合成、数据增强、跨域迁移等应用的新一轮热潮。
    
5. 当前深度学习正沿着“更深、更轻量化、更可控”的方向演进：ResNet、Transformer、Diffusion Model、NeRF 等结构不断涌现，与强化学习、图神经网络、自监督学习结合，持续拓展人工智能的边界。